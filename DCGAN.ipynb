{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DCGAN实战测试\n",
    "# 这里采用 matplotlib.image 读入图片数组，注意这里读入的数组是 float32 型的，范围是 0-1\n",
    "# https://blog.csdn.net/qq_36758914/article/details/104878227 这个是一个详细说明的版本\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "import multiprocessing\n",
    "img_paths = glob.glob('./data/images/*.jpg')\n",
    "# batch_size = 4\n",
    "# print(len(img_path)) 63565\n",
    "# 构建自定义数据集\n",
    "# https://www.cnblogs.com/heze/p/12390926.html\n",
    "def make_anime_dataset(img_paths,batch_size,resize=64,drop_remainder=True,shuffle=True,repeat=1):\n",
    "    @tf.function\n",
    "    def _map_fn(img):\n",
    "        img = tf.io.read_file(img)\n",
    "        img = tf.image.decode_jpeg(img,channels=3)\n",
    "        img = tf.image.resize(img,[resize,resize])\n",
    "        img = tf.clip_by_value(img,0,255)\n",
    "        img = img/127.5-1\n",
    "        return img\n",
    "    img_shape = (resize, resize, 3)\n",
    "    len_dataset = len(img_paths) # batch_size\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(img_paths)\n",
    "    dataset = dataset.shuffle(1000).map(_map_fn).batch(batch_size)\n",
    "    # print(dataset)\n",
    "    # print(iter(dataset).next())\n",
    "    return dataset, img_shape, len_dataset\n",
    "    \n",
    "# https://ithelp.ithome.com.tw/articles/10241789?sc=rss.iron dataset用法参考\n",
    "# https://ithelp.ithome.com.tw/articles/10268970     Shuffle Batch Repeat\n",
    "# https://www.cnblogs.com/marsggbo/p/9603789.html\n",
    "\n",
    "#     # drop_remainder https://www.cnblogs.com/wkslearner/p/9484443.html （这个也是参数详解）\n",
    "#     dataset = dataset.repeat(repeat).prefetch(n_prefetch_batch)\n",
    "#     # https://zhuanlan.zhihu.com/p/163656225 prefetch\n",
    "\n",
    "#   https://blog.csdn.net/xierhacker/article/details/79002902 from_tensor_slices\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.api._v2.keras as keras\n",
    "from keras.api._v2.keras import layers\n",
    "class Generator(keras.Model):\n",
    "    # 生成器网络\n",
    "    def __init__(self):\n",
    "        super(Generator,self).__init__()\n",
    "        filter = 64\n",
    "\n",
    "        # 转置卷积层1，输出channel为filter*8，核 4，s 1,padding 0,无偏置\n",
    "        self.conv1 = layers.Conv2DTranspose(filter*8,4,1,'valid',use_bias=False)\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "\n",
    "        # 转置卷积层2\n",
    "        self.conv2 = layers.Conv2DTranspose(filter*4,4,2,'same',use_bias=False)\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "\n",
    "        # 转置卷积层3\n",
    "        self.conv3 = layers.Conv2DTranspose(filter*2,4,2,'same',use_bias=False)\n",
    "        self.bn3 = layers.BatchNormalization()\n",
    "\n",
    "        # 转置卷积层4\n",
    "        self.conv4 = layers.Conv2DTranspose(filter*1,4,2,'same',use_bias=False)\n",
    "        self.bn4 = layers.BatchNormalization()\n",
    "\n",
    "        # 转置卷积层5\n",
    "        self.conv5 = layers.Conv2DTranspose(3,4,2,'same',use_bias=False)\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        x = inputs\n",
    "        x = tf.reshape(x,(x.shape[0],1,1,x.shape[1]))\n",
    "        x = tf.nn.relu(x)\n",
    "\n",
    "        x = tf.nn.relu(self.bn1(self.conv1(x),training=training))\n",
    "\n",
    "        x = tf.nn.relu(self.bn2(self.conv2(x),training=training))\n",
    "\n",
    "        x = tf.nn.relu(self.bn3(self.conv3(x),training=training))\n",
    "\n",
    "        x = tf.nn.relu(self.bn4(self.conv4(x),training=training))\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        \n",
    "        x = tf.tanh(x) #输出x范围-1 ~1,与预处理一致\n",
    "        # print(x)\n",
    "        return x\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 判别器\n",
    "class Discriminator(keras.Model):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Discriminator,self).__init__()\n",
    "        filter = 64\n",
    "\n",
    "        self.conv1 = layers.Conv2D(filter,4,2,'valid',use_bias=False)\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "\n",
    "        self.conv2 = layers.Conv2D(filter*2,4,2,'valid',use_bias=False)\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "\n",
    "        self.conv3 = layers.Conv2D(filter*4,4,2,'valid',use_bias=False)\n",
    "        self.bn3 = layers.BatchNormalization()\n",
    "\n",
    "        self.conv4 = layers.Conv2D(filter*8,3,1,'valid',use_bias=False)\n",
    "        self.bn4 = layers.BatchNormalization()\n",
    "\n",
    "        self.conv5 = layers.Conv2D(filter*16,3,1,'valid',use_bias=False)\n",
    "        self.bn5 = layers.BatchNormalization()\n",
    "\n",
    "        # 全局池化层\n",
    "        self.pool = layers.GlobalAveragePooling2D()\n",
    "\n",
    "        # 特征打平层\n",
    "        self.flatten = layers.Flatten()\n",
    "\n",
    "        # 分类全连接层\n",
    "        self.fc = layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "            # （b，31，31，64）\n",
    "        x = tf.nn.leaky_relu(self.bn1(self.conv1(inputs),training=training))\n",
    "        # （b，14,14，128）\n",
    "        \n",
    "        x = tf.nn.leaky_relu(self.bn2(self.conv2(x),training=training))\n",
    "        # （b，6，6，256）\n",
    "        x = tf.nn.leaky_relu(self.bn3(self.conv3(x),training=training))\n",
    "        # （b，4，4，512）\n",
    "        x = tf.nn.leaky_relu(self.bn4(self.conv4(x),training=training))\n",
    "        # （b，2，2，1024）\n",
    "        x = tf.nn.leaky_relu(self.bn5(self.conv5(x),training=training))\n",
    "        # （b，1024）\n",
    "        x = self.pool(x)\n",
    "\n",
    "        logits = self.fc(x)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def celoss_ones(logits):\n",
    "    y = tf.ones_like(logits)\n",
    "    loss = keras.losses.binary_crossentropy(y,logits,from_logits=True)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "def celoss_zeros(logits):\n",
    "    y = tf.zeros_like(logits)\n",
    "    loss = keras.losses.binary_crossentropy(y,logits,from_logits=True)\n",
    "    return tf.reduce_mean(loss)\n",
    "# 判别器误差损失函数\n",
    "\n",
    "def d_loss_fn(generator,discriminator,batch_z,batch_x,is_training):\n",
    "    # 采样生成图片\n",
    "    fake_image = generator(batch_z,is_training)\n",
    "    # print(fake_image)\n",
    "    # print(\"generator\")\n",
    "    # 判定该生成图片\n",
    "    d_fake_logits = discriminator(fake_image,is_training)\n",
    "\n",
    "    # 判断真实图片\n",
    "    d_real_logits = discriminator(batch_x,is_training)\n",
    "\n",
    "    # 真实图片与1的误差\n",
    "    d_loss_real = celoss_ones(d_real_logits)\n",
    "\n",
    "    # 生成图片与0之间的误差\n",
    "    d_loss_fake = celoss_zeros(d_fake_logits)\n",
    "\n",
    "    return d_loss_fake + d_loss_real\n",
    "\n",
    "# 生成器网络损失函数\n",
    "\n",
    "def g_loss_fn(generator,discriminator,batch_z,is_training):\n",
    "    # 采样生成图片\n",
    "    fake_image = generator(batch_z,is_training)\n",
    "    # 在训练网络时需要迫使生成图片判断为真\n",
    "    d_fake_logits = discriminator(fake_image,is_training)\n",
    "    # 计算生成图片与1之间的误差\n",
    "    loss = celoss_ones(d_fake_logits)\n",
    "\n",
    "    return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "# import scipy.misc\n",
    "def save_result(val_out, val_block_size, image_path, color_mode):\n",
    "      def preprocess(img):\n",
    "          img = ((img + 1.0) * 127.5).astype(np.uint8)\n",
    "          # img = img.astype(np.uint8)\n",
    "          return img\n",
    "  \n",
    "      preprocesed = preprocess(val_out)\n",
    "      final_image = np.array([])\n",
    "      single_row = np.array([])\n",
    "  \n",
    "      for b in range(val_out.shape[0]):\n",
    "          # concat image into a row\n",
    "          if single_row.size == 0:\n",
    "              single_row = preprocesed[b, :, :, :]\n",
    "          else:\n",
    "              single_row = np.concatenate((single_row, preprocesed[b, :, :, :]), axis=1)\n",
    "  \n",
    "          # concat image row to final_image\n",
    "          if (b + 1) % val_block_size == 0:\n",
    "              if final_image.size == 0:\n",
    "                  final_image = single_row\n",
    "              else:\n",
    "                  final_image = np.concatenate((final_image, single_row), axis=0)\n",
    "  \n",
    "              # reset single row\n",
    "              single_row = np.array([])\n",
    "  \n",
    "      if final_image.shape[2] == 1:\n",
    "          final_image = np.squeeze(final_image, axis=2)\n",
    "      im = Image.fromarray(final_image)\n",
    "      im.save(image_path)\n",
    "    #   Image.save(final_image)\n",
    "    #   Image(final_image).save()\n",
    "  \n",
    "  \n",
    "# d_losses, g_losses = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 网络训练\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "z_dim = 100\n",
    "epochs = 300\n",
    "batch_size = 64\n",
    "is_training = True\n",
    "\n",
    "generator = Generator()\n",
    "\n",
    "\n",
    "generator.build(input_shape=(4,z_dim))\n",
    "# 这个4随便设一个就好\n",
    "\n",
    "discriminator = Discriminator()\n",
    "\n",
    "discriminator.build(input_shape=(4,64,64,3))\n",
    "\n",
    "g_optimizer = keras.optimizers.Adam(learning_rate=2e-4,beta_1=0.5)\n",
    "d_optimizer = keras.optimizers.Adam(learning_rate=2e-4,beta_1=0.5)\n",
    "dataset,img_shape,_ = make_anime_dataset(img_paths,batch_size,resize=64)\n",
    "\n",
    "db_iter = iter(dataset)\n",
    "\n",
    "\n",
    "# 测试\n",
    "# test = tf.random.normal([64,100])\n",
    "\n",
    "# test = generator(test,True)\n",
    "# print(test)\n",
    "# test = test.numpy()\n",
    "# test = ((test + 1.0) * 127.5).astype(np.uint8)\n",
    "# plt.imshow(test[0])\n",
    "# plt.show()\n",
    "# print(next(db_iter))\n",
    " # 可视化\n",
    "# z = tf.random.normal([100, z_dim])\n",
    "# fake_image = generator(z, training=False)\n",
    "# img_path = os.path.join('gan_images', 'gan-%d.png'%1)\n",
    "# save_result(fake_image.numpy(), 10, img_path, color_mode='P')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard\n",
    "import datetime\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\n",
    "test_log_dir = 'logs/gradient_tape/' + current_time + '/test'\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "test_summary_writer = tf.summary.create_file_writer(test_log_dir)\n",
    "\n",
    "generator_loss = tf.keras.metrics.Mean('generator_loss',dtype=tf.float32)\n",
    "discriminator_loss = tf.keras.metrics.Mean('discriminator_loss',dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:{},step:{} 0 0\n",
      "epoch:{},step:{} 0 1\n",
      "epoch:{},step:{} 0 2\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):  # 训练epochs 次\n",
    "    for i,batch_x in enumerate(dataset):\n",
    "         # 1. 训练判别器\n",
    "         print('epoch:{},step:{}',epoch,i)\n",
    "         # 采样隐藏向量\n",
    "         batch_z = tf.random.normal([batch_size, z_dim])\n",
    "         # 采样真实图片\n",
    "         # 判别器前向计算\n",
    "         with tf.GradientTape() as tape:\n",
    "             d_loss = d_loss_fn(generator, discriminator, batch_z, batch_x, is_training)\n",
    "         grads = tape.gradient(d_loss, discriminator.trainable_variables)\n",
    "         d_optimizer.apply_gradients(zip(grads, discriminator.trainable_variables))\n",
    "        \n",
    "         discriminator_loss(d_loss)\n",
    "         # 2. 训练生成器\n",
    "        #  # 采样隐藏向量\n",
    "        #  batch_z = tf.random.normal([batch_size, z_dim])\n",
    "        \n",
    "         # 生成器前向计算\n",
    "         with tf.GradientTape() as tape:\n",
    "             g_loss = g_loss_fn(generator, discriminator, batch_z, is_training)\n",
    "         grads = tape.gradient(g_loss, generator.trainable_variables)\n",
    "         g_optimizer.apply_gradients(zip(grads, generator.trainable_variables))\n",
    "         \n",
    "         generator_loss(g_loss)\n",
    "         \n",
    "    with train_summary_writer.as_default():\n",
    "       tf.summary.scalar('generator_loss',generator_loss.result(),step = epoch)\n",
    "       tf.summary.scalar('discriminator_loss',discriminator_loss.result(),step = epoch)\n",
    "\n",
    "    template = 'Epoch {}, g_Loss: {},d_loss:{}'\n",
    "    print(template.format(epoch+1,generator_loss.result(),discriminator_loss.result()))\n",
    "       # https://zhuanlan.zhihu.com/p/84215973  tf2 tensorboard教程\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "      print(epoch, 'd-loss:', float(d_loss), 'g-loss:', float(g_loss))  # 可视化\n",
    "      z = tf.random.normal([100, z_dim])\n",
    "      fake_image = generator(z, training=False)\n",
    "      img_path = './gan_images'\n",
    "      img_path = os.path.join(img_path, 'gan-%d.png' % epoch)\n",
    "      save_result(fake_image.numpy(), 10, img_path, color_mode='P')\n",
    "    \n",
    "    generator_loss.reset_states()\n",
    "    discriminator_loss.reset_states()\n",
    "        #    if epoch % 10000 == 1:\n",
    "        #        # print(d_losses)\n",
    "        #        # print(g_losses)\n",
    "        #        generator.save_weights('exam11.1_generator.ckpt')\n",
    "        #        discriminator.save_weights('exam11.1_discriminator.ckpt')\n",
    "\n",
    "\n",
    "\n",
    "# generator = keras.Sequential()\n",
    "# generator.add(Generator())\n",
    "\n",
    "# for epoch in range(10):\n",
    "#     for _ in range(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40521ad78ffc87a0a85110f7601201e52b9180c55ac867d59967a89a1791ab63"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
